\documentclass[]{article}

\usepackage[math]{kurier}
\usepackage[sc]{mathpazo}                   
\renewcommand{\sfdefault}{kurier}

%I added this to the template
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{authblk}
\usepackage{tabularx}

\usepackage{graphics}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}


\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf \sffamily AA 274: Principles of Robotic Autonomy
                        \hfill Winter 2018} }
       \vspace{4mm}
       \hbox to 6.28in { {\sffamily{\Large \hfill Lecture #1: #2  \hfill}} }
       \vspace{2mm}
       \hbox to 6.28in { {\it \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace*{4mm}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%
%document
\begin{document}
%modify this
\lecture{5}{Camera Models and Camera Calibration}{}{Nathan Usevitch, Vik Pattabi, Huajian Huang, Zaifeng Zheng, Lucy Zhuang}

%%%%%%% Beginning of Vik's Section
\section{Introduction}
This lecture is on camera models and camera calibration. More specifically, we want to learn:
\begin{itemize}
  \item What a camera model is, and how to use it.
  \item How to calibrate a camera.
  \item Other challenges in image processing.
\end{itemize}

\section{Perspective Projection}
Our goal is \textbf{to determine how points in the world map to points in our camera image}. We start with an assumption of the pinhole camera model (note that all following results also hold when the thin lens model is assumed if we set the camera to be focused at $\infty$ ). 

\begin{figure}[H]
\includegraphics[width=0.4\textwidth]{diagram_1_vik.png}
\centering
\caption{Recall the pinhole camera model discussed in the previous lecture. Our goal is transform coordinates in the world frame $O_w$ to coordinates in the Image Plane. }
\label{fig:pinhole_cam}
\end{figure}

\textbf{Step 1}\\\\
First, we aim to map our point $P_c$ in the camera frame to a point \textit{p} = (\textit{x}, \textit{y}). Recall from the previous lecture:

\begin{center}
\begin{equation}
\begin{tabular}{c}
  $x = f\frac{X_C}{X_C}$\\
  \\
  $y = f\frac{Y_C}{X_C}$
\end{tabular}
\end{equation}
\end{center}

\textbf{Step 2.a}\\\\
Now, we must recognize that the actual origin of a camera coordinate system is not centered on the image plane, but rather usually in the lower right or lower left corner. This is typically done so as to avoid the need for negative coordinates (and consequently, negative array indices) when accessing a specific point on the image plane, which is typically represented as a grid or series of arrays in software.

\begin{figure}[H]
\includegraphics[width=0.4\textwidth]{vik_image_2.png}
\centering
\caption{Here, we place our origin in the lower right corner of the image plane. }
\label{fig:camera_coordinates}
\end{figure}

Given this coordinate translation, it's evident that our old point can be represented in the new axes as:

\begin{center}
\begin{equation}
\begin{tabular}{c}
  $\tilde{x} = f\frac{X_C}{Z_C} + \tilde{x}_0$\\
  \\
  $\tilde{y} = f\frac{Y_C}{Z_C} + \tilde{y}_0$
\end{tabular}
\end{equation}
\end{center}

\textbf{Step 2.b}\\\\
Another issue we must account for is that the ratio of image coordinates to actual pixels is likely not one-to-one. In other words, a shift of one unit in our image plane may correspond to a shift of multiple pixels. Consequently, we introduce $k_x$ and $k_y$ which represent the number of pixels per unit distance in the x and y direction respectively. Although it's certainly possible that $k_x = k_y$, typically this will not be the case if the camera pixels are not square. From this intuition, it follows that...

\begin{center}
\begin{equation}
\begin{tabular}{c}
  $u = k_x\tilde{x} = k_xf\frac{X_C}{Z_C} + k_x\tilde{x}_0$\\
  \\
  $v = k_y\tilde{y} = k_yf\frac{Y_C}{Z_C} + k_y\tilde{y}_0$
\end{tabular}
\end{equation}
\end{center}

We can simplify our result by renaming the following expressions...
\begin{center}
\begin{tabular}{c}
$k_xf = \alpha$\\
\\
$k_yf = \beta$\\
\\
$k_x\tilde{x}_0 = u_0$\\
\\
$k_y\tilde{y}_0 = v_0$
\end{tabular}
\end{center}

Giving us a final result of ...

\begin{center}
\begin{equation}
\begin{tabular}{c}
  $u = \alpha\frac{X_C}{Z_C} + u_0$\\
  \\
  $v = \beta\frac{Y_C}{Z_C} + v_0$
\end{tabular}
\end{equation}
\end{center}

\textbf{Homogeneous Coordinates}\\\\
We now note that our transformations from camera frame to pixel value are, unfortunately, not linear. If we can represent these transformations as a linear mapping, we will be able to greatly simplify our math later on. Thus, we introduce the idea of homogeneous coordinates. Broadly defined, the homogeneous coordinates ($x_1$, $x_2$, $x_3$) for a point ($x$, $y$) are any three points such that $\frac{x_1}{x_3} = x$ and $\frac{x_2}{x_3} = y$.\\\\ Consequently, we use the following rules to transform from inhomogeneous coordinates to homogeneous ones.\\\\
\begin{center}
$\begin{pmatrix} 
x \\
y \\
\end{pmatrix}
$
$\Rightarrow$
$\lambda\begin{pmatrix} 
x \\
y \\
1
\end{pmatrix}
$
and 
$\begin{pmatrix} 
x \\
y \\
z
\end{pmatrix}
$
$\Rightarrow$
$\lambda\begin{pmatrix} 
x \\
y \\
z \\
1
\end{pmatrix}
$
\end{center}
And we use these rules to transform back from homogeneous coordinates to inhomogeneous ones. \\\\
\begin{center}
$\lambda\begin{pmatrix} 
x \\
y \\
w
\end{pmatrix}
$
$\Rightarrow$
$\begin{pmatrix} 
x/w \\
y/w \\
\end{pmatrix}
$
and 
$\lambda\begin{pmatrix} 
x \\
y \\
z \\
w
\end{pmatrix}
$
$\Rightarrow$
$\begin{pmatrix} 
x/w \\
y/w \\
z/w
\end{pmatrix}
$
\end{center}
Note that as we continue to use homogeneous coordinates, we might signal that a specific coordinate matrix is in homogeneous form by using a superscript $h$ (e.g. $P^h$)



%%%%%% Beginning of Lucy's Section %%%%%%%%%
\section{Perspective Projection In Homogeneous Coordinates}

When we shift to representing our points in homogeneous coordinates, we arrive at the following result; note that this is equivalent to the set of equations we derived earlier which translated $P_C$ to the Image Plane, but now $P_C$ is homogeneous.
\begin{equation}
\begin{bmatrix} 
\alpha & 0 & u_{0} & 0 \\
0 & \beta & v_{0} & 0  \\
0 & 0 & 1 & 0
\end{bmatrix}
\begin{pmatrix} 
X_{c} \\
Y_{c} \\
Z_{c} \\
1
\end{pmatrix}
=
\begin{pmatrix} 
\alpha X_{c} + u_{0}Z_{c} \\
\beta Y_{c} + v_{0}Z_{c} \\
Z_{c} \\
\end{pmatrix}
\end{equation}
$\begin{pmatrix} 
X_{c} \\
Y_{c} \\
Z_{c} \\
1
\end{pmatrix}
$
is our pixel $P_C$ in homogeneous coordinates; we will ultimately map these coordinates to $P_W$ or world coordinates.
The preceding 3x4 matrix is called $K$, and is alternatively referred to as the camera matrix or matrix of intrinsic parameters. $K$ contains the fundamental characteristics of our given pinhole camera. Although we may have access to these values from the manufacturer specs, we often have to re-calibrate the camera to account for differences in tolerances or noise.

The $K$ depicted in the above equation is typically standard. However, in some rare cases, the Image Plane coordinate axes may not be perpendicular, forcing us to introduce another variable, $\gamma$, representing this skew parameter Typically, we treat $\gamma$ as 0.
\begin{equation}
K = 
\begin{bmatrix} 
\alpha & \gamma & u_{0} & 0 \\
0 & \beta & v_{0} & 0  \\
0 & 0 & 1 & 0
\end{bmatrix}
\end{equation}
Now, our next step is mapping between the world frame and digital camera frame. 

\begin{figure}[H]
\includegraphics[width=0.4\textwidth]{vik_image_3.png}
\centering
\caption{Here, we depict our point ($P_W$ or $P_C$) relative to the origins of the camera frame and world frame ($O$ and $O_W$ respectively). }
\label{fig:camera_coordinates}
\end{figure}

From this depiction, we have the following:
\begin{equation}
\label{eq:ising}
P_{C} = t + q
\end{equation}
We express $P$ in the camera reference through vector addition. $t$ is the vector from the origin of camera frame to the origin of the world frame. $q$ is the vector representing the position of $P$ with respect to the world frame.\\\\
How do we determine $q$ in relation to the camera frame instead of the world frame? We say:
\begin{equation}
\label{eq:ising}
q = RP_{w}
\end{equation}
where R is the rotation matrix relating camera frame to world frame. Now, we must determine $R$. We start by re-expressing our vector $q$, as seen below:
\begin{equation}
\label{eq:1}
q = q_{x,c} \cdot \hat{i} + q_{y,c} \cdot \hat{j} + q_{z,c} \cdot \hat{k}
\end{equation}
The above equation expresses q with relation to the camera frame.
At the same time, $q$ can also be expressed with respect to the world frame:
\begin{equation}
\label{eq:ising}
q = q_{x,w} \cdot \hat{i_{w}} + q_{y,w} \cdot \hat{j_{w}} + q_{z,w} \cdot \hat{k_{w}}
\end{equation}

We know these two expressions refer to the same $q$, so:
\begin{equation}
\label{eq:ising}
q_{x,w} \cdot \hat{i_{w}} + q_{y,w} \cdot \hat{j_{w}} + q_{z,w} \cdot \hat{k_{w}} = q_{x,c} \cdot \hat{i} + q_{y,c} \cdot \hat{j} + q_{z,c} \cdot \hat{k}
\end{equation}

Now, for example, we might take the dot product of both sides with the unit vector $\hat{i}$. Because the dot product of orthogonal vectors is 0, we are able to eliminate some terms and are left with the following:
\begin{equation}
q_{x,c} = q_{x,w}\hat{i_w} \cdot \hat{i} + q_{y,w}\hat{j_w} \cdot \hat{i} + q_{z,w}\hat{k_w} \cdot \hat{i}
\end{equation}\\
The derivation of the the other coordinates between world and camera frame is similar, and we get the following rotation matrix:

\begin{equation}
R = 
\begin{bmatrix} 
\hat{i_w} \cdot \hat{i} & \hat{j_w} \cdot \hat{i} & \hat{k_w} \cdot \hat{i} \\
\hat{i_w} \cdot \hat{j} & \hat{j_w} \cdot \hat{j} & \hat{k_w} \cdot \hat{j} \\
\hat{i_w} \cdot \hat{k} & \hat{j_w} \cdot \hat{k} & \hat{k_w} \cdot \hat{k}
\end{bmatrix}
\end{equation}

These sorts of matrices are common in image processing and camera-related tasks.

%%%%%%% Beginning of Nathan's Section
\section{Finalizing the Transformation}
We are able to express the rigid transformation containing both rotation and translation between a point in the world frame to a point in the camera frame by using homogeneous coordinates as follows: 

\begin{equation}
\begin{bmatrix}
P_c \\
1
\end{bmatrix}
=
\begin{bmatrix}
R && t \\
0_{3x0} && 1
\end{bmatrix}
\begin{bmatrix}
P_w \\ 1
\end{bmatrix}
\end{equation}

We can combine the transform between a point in the world frame to a point in the camera frame and the transform from a point in the camera frame to a point in the image frame to generate the transform from the the world frame directly to the image frame:
 
 \begin{equation}
p^h
=
K \begin{bmatrix}
I_{3x3} \quad 0_{3x1}
\end{bmatrix}
\begin{bmatrix}
R && t \\
0_{3x0} && 1
\end{bmatrix}
P_w^h
 \end{equation}
 
 \begin{equation}
 p^h = K[R \quad t] P_W^h.
 \end{equation}

Here the matrix $K$ is the matrix of intrinsic parameters, and the $[R \quad t]$ matrix contains the extrinsic parameters, which describe the cameras relationship to the points in the photograph. The superscript $h$ means the point is expressed in homogeneous coordinates, meaning that $p^h$ is the $3x1$ homogeneous coordinates matrix of the point in the image frame, and $P_w^h$ is the $4x1$ homogeneous coordinates matrix of the point in the world frame. 

The total degrees of freedom for the combined matrices are 5 for $K$, 3 for the rotation matrix $R$ and 3 for the translation $t$, leading to a total of 11 parameters. Note that the rotation matrix $R$ has 3 DOF because rotation of a coordinate frame takes place in three linearly independent planes. If we assume that our skewness $\gamma$ is 0, K has 4 parameters and the total number of degrees of freedom is 10 instead.

This perspective projection naturally causes several consequences which can be mathematically derived. For example, parallel lines in images appear to meet at the horizon, and objects that are far away appear smaller than their closer counterparts. Both effects are shown in the following image of train tracks.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{traintracks.jpg}
\centering
\caption{An image of train tracks. Note that the parallel tracks appear to meet at the horizon. The wooden railroad ties (perpendicular to the tracks) are all the same size, but those further from the camera appear smaller than the closer railroad ties. }
\label{fig:train_tracks}
\end{figure}


\subsection{Direct Linear Calibration}
The problem of direct linear calibration is the following: given known correspondences between known points in the camera frame $(p_i)$ and known points in the world frame $P_{W,i}$, determine the intrinsic parameters $(K)$ and the extrinsic parameters $(R,t)$ of the camera.

A common method of determining these known correspondences is to use images of a grid with a chessboard like pattern. Such a pattern enables simple extraction of the corners, and identification of relationships between objects in the world frame. 

%%%%%%%% Beginning of Huajian's Section
\section{Solving Calibration Problems}

The major idea in camera calibration is that, given some known positions in the image frame, and the corresponding world frame positions, we back out the components of the matrices in the previous sections. These matrices include (1) the intrinsic parameter matrix, (2) the rotation matrix from image frame to world frame, and (3) the vector connecting the world frame and image frame.\\

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{vik_image_4.png}
\centering
\caption{An example of camera calibration. Here, we know the relationship between the red point $P_{W,i}$ in the world and $p_i$ in the image plane.}
\label{fig:calibration}
\end{figure}


First of all, we have the fundamental relationships, relating $p_i^h$, the projected point on the image plane in homogeneous coordinates, and $P_{W,i}^h$ the corresponding point in the world frame in homogeneous coordinates:
\begin{equation}
    p_i^h = M P_{W,i}^{h}\hspace{1mm}, \hspace{3mm}i = 1,....n
\end{equation}

where 
\begin{equation}
    M = K [R \hspace{2mm} t]= 
\begin{bmatrix}
    m_1 \\
    m_2 \\
    m_3
\end{bmatrix}
\end{equation}




The process involves feeding in $n$ corresponding pairs of image frame positions and world frame positions, where $i$ is the index of the current pair. The $M$ matrix is size 3 by 4, and each $m_i$ is size 1 by 4. Note that we are working with homogeneous coordinates. We have: 
\begin{equation}
    p_i^h = [\lambda u \hspace{2mm} \lambda v \hspace{2mm} \lambda]^T
\end{equation}
\begin{equation}
        P_{W,i}^{h} = [x \hspace{2mm} y \hspace{2mm} z \hspace{2mm} 1]^T
\end{equation}
which conforms with the matrix multiplication rules given $M$ is 3 by 4.\\


If we write out the relationships between a pair of $p_i^h$ and $P_{W,i}^h$, we can obtain the following:
\begin{equation}
    \lambda = m_3 \cdot P_{W,i}^h
\end{equation}

\begin{equation}
    u_i = \frac{\lambda u_i}{\lambda} = \frac{m_1 \cdot P_{W,i}^h}{m_3 \cdot P_{W,i}^h}
\end{equation}
\begin{equation}
    v_i = \frac{\lambda v_i}{\lambda} = \frac{m_2 \cdot P_{W,i}^h}{m_3 \cdot P_{W,i}^h}
\end{equation}

which can be transformed into the form of two constraint equations:
\begin{equation}\label{eqn:u_contraint}
    u_i(m_3 \cdot P_{W,i}^h) - m_1 \cdot P_{W,i}^h = 0
\end{equation}
\begin{equation}
    v_i(m_3 \cdot P_{W,i}^h) - m_2 \cdot P_{W,i}^h = 0
\end{equation}

Now if we stack all of these (n pairs of these $u_i$ and $v_i$ equations) together, we can obtain a system of equations, and to write them compactly, we can use matrix multiplication:
\begin{equation}
    \tilde{P}m = 0, \hspace{5mm} m = 
    \begin{bmatrix}
    m_1^T \\
    m_2^T \\
    m_3^T
\end{bmatrix}
\end{equation}

Let's write out a few rows of $\tilde{P}$ that correspond to the $u_i$ and $v_i$ constraints above to make things more concrete:


\begin{equation}
\tilde{P} = 
     \begin{bmatrix}
  -(P_{W,1}^h)^T & 0_{1x4} & u_{1} (P_{W,1}^h)^T \\
   0_{1x4} & -(P_{W,1}^h)^T &v_{1} (P_{W,1}^h)^T  \\
  -(P_{W,2}^h)^T & 0_{1x4} & u_{2} (P_{W,2}^h)^T \\
  \vdots & \vdots & \vdots
 \end{bmatrix}
\end{equation}



The size of $\tilde{P}$ is 2n by 12, and the size of $m$ is 12 by 1.


Now, we ideally want to directly solve this system of equations and obtain the unique set of $m_i$ values, which should allow us to obtain all parameters of the camera. We have 12 parameters to determine (including the scaling parameter for the projection). Potentially, we might want to feed in more pairs of corresponding positions to make this process more robust; however, that would result in more equations than coefficients to solve for, making the problem "over-constrained". \\\\
Furthermore, due to the uncertainties which arise in obtaining any of these positions data, it's very likely that this system of equations is not perfectly satisfied. Therefore, we will frame this as a minimization problem where we try to satisfy this system of equations as well as possible, with a non-trivial $m$ coefficient vector. Note that, without the constraint of square norm of $m$ equals 1, the minimization would result in trivial coefficients (all $m_i$ being zeros). 

\begin{equation}
    min_{m\in R^{12}} \hspace{3mm} \| \tilde{P} m \|^2
\end{equation}
subject to 
\begin{equation}
    \| m \|^2 = 1
\end{equation}

This choice of the norm of $m$ relies on the fact the a scalar multiple of the solution is still a solution of the original system of equations. Now, let's construct the Lagrangian first, by adding the cost and the Lagrange multiplier times the constraint:
\begin{equation}
    L = m^T\tilde{P}^T\tilde{P}m + \lambda(1 - m^T m)
\end{equation}
We take the gradient and set it to zero as the necessary condition of optimality:
\begin{equation}
    \nabla L = 2\tilde{P}^T \tilde{P} m - 2\lambda m = 0  \xrightarrow{} \tilde{P}^T \tilde{P} m = \lambda m
\end{equation}

Now, it's obvious that the $m$ we are looking for is an eigenvector of the $\tilde{P}^T \tilde{P}$. Furthermore, it is the eigenvector that has the smallest eigenvalue that would minimize the square norm. 


After obtaining the $M$ matrix upon solving the minimization problem, we will factorize $M$ as 
\begin{equation}
    M = [KR \hspace{2mm} Kt]
\end{equation}
The first 3 columns of the matrix are the product of the intrinsic parameter matrix, which is upper triangular, and the rotation matrix, which is orthogonal. This calls for the efficient RQ factorization method, and we can obtain the parameters from the corresponding entries. 


%%%%%%% Beginning of Zaifeng's Section

\section{Radial Distortion}
So far, there's another problem not being considered: radial distortion. For a real lens, the linear assumption will not hold any more. Either barrel distortion or pincushion distortion will exist and affect the real pixel coordinates.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{distortion.png}
\centering
\caption{Different kinds of distortions}
\label{fig:distortion}
\end{figure}

There are many algorithms that address image distortion. A simple and efficient way is to set up relations between the ideal pixel position $(u,v)$ and the distorted pixel position $(u_d,v_d)$

\begin{equation}
\begin{bmatrix}
    u_d \\
    v_d
\end{bmatrix}
=
\begin{bmatrix}
    u_d \\
    v_d
\end{bmatrix}
(1+kr^2)
\begin{bmatrix}
    u-u_{cd} \\
    v-v_{cd}
\end{bmatrix}
+
\begin{bmatrix}
    u_{cd} \\
    v_{cd}
\end{bmatrix}
\end{equation}

where $k$ is the radial distortion factor that differs in different cameras and needs to be pre-calculated. $r^2=(u-u_{cd})^2+(v-v_{cd})^2$ is the square of the distance between the ideal pixel location and the center of distortion. $(u_{cd},v_{cd})$ are the coordinates of the image center.

Although this algorithm is efficient, there are many more sophisticated tools which we will not address.

\section{Measuring Depth}
Now, once we know our parameters $K,R,t$, can we find out the corresponding coordinates in the real world if $p$ on our image plane is known? 

Unfortunately, the answer is no. That's because the equation $p^h=K[R t]P_W^h$ is not revertible. We still need to decide an extra variable in the 3D world - the depth of $P$, $Z_W$. The only thing we are sure now is that P lies along the line connecting $O$ and $p$. 

\begin{figure}[H]
\includegraphics[width=0.3\textwidth]{measuring_depth.png}
\centering
\caption{The depth of the real object is not known yet.}
\label{fig:measuring_depth}
\end{figure}

\section{Recovering Structure}
The previous discussion tells us that we can not simply determine the depth/structure from the pixel coordinates and camera parameters. We see this effect when we look at the picture below. We only know the man is not really stepping on the tower because of our own contextual clues or experience, not from the picture itself.

\begin{figure}[H]
\includegraphics[width=0.3\textwidth]{stepping_on_tower.png}
\centering
\caption{In the picture, the man seems to be stepping on the tower.}
\label{fig:stepping_on_tower}
\end{figure}

However, we can achieve the goal using other tools. This process is usually called structure recovering - we aim to reconstruct a 3D scene only with access to 2D images. Some common methods are listed below.

\begin{itemize}
  \item Through	recognition of landmarks (e.g., orthogonal walls)
  \item Depth from focus: determines distance to one point by taking multiple images with better and better focus
  \item Stereo vision: processes two distinct images taken at the same time and assumes that the relative pose between the two cameras is known
  \item Structure from motion: processes two images taken with the same or different cameras at different times and from different unknown positions
\end{itemize}

\section{References}
Relevant readings related to or referenced in the lecture include:
\begin{itemize}
  \item SNS: 4.2.3
  \item D.A.Forsyth	and	J.	Ponce[FP]. Computer Vision:	A	Modern	Approach	(2nd	
Edition).	Prentice	Hall,	2011.	Chapter	1.
  \item R.	Hartley	and	A.	Zisserman	[HZ].	Multiple	View	Geometry	in	Computer	
Vision.	Academic	Press,	2002.	Chapter	6.1.
  \item Z.	Zhang.	A	Flexible	New	Technique	for	Camera	Calibration. IEEE	Transactions	
on	Pattern	Analysis	and	Machine	Intelligence,	2000.
\end{itemize}

\end{document}