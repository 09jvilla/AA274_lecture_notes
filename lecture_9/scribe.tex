\documentclass[twoside]{article}

\usepackage{enumitem} % for customizing enumerate tags
\usepackage{amsmath,amsthm,amssymb}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{minted}
\usepackage[math]{kurier}
\usepackage[sc]{mathpazo}                   
\renewcommand{\sfdefault}{kurier}


\usepackage{graphics}
\usepackage{graphicx}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}




\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf \sffamily AA 274: Principles of Robotic Autonomy
                        \hfill Winter 2018} }
       \vspace{4mm}
       \hbox to 6.28in { {\sffamily{\Large \hfill Lecture #1: #2  \hfill}} }
       \vspace{2mm}
       \hbox to 6.28in { {\it \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace*{4mm}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%
%document
\begin{document}
%modify this
\lecture{9}{Machine Learning and Modern Visual Recognition Techniques}{}{Chi Zhang, Honghao Wei, Matthew Tan, Taylor Howell, Brian Jackson, Saifan Rafiq}
%%%%%%%%%%%
%  PART I %
%%%%%%%%%%%
\section{Machine Learning Overview}
\subsection{Supervised vs. Unsupervised Learning}
Machine learning is a field of computer science that gives computers the ability to learn without being explicitly programmed \cite{wiki-machinelearning}. Problems in this field can generally be categorized as either supervised or unsupervised learning. The goal of supervised learning is to take a set of labeled training data and learn the function that best predicts the label of a new, previously unseen, input. Classic examples include image classification (i.e. whether or not a picture contains a cat) or predicting the selling prices of houses as a function of various features, such as location, number of bedrooms, square footage, etc. The goal of unsupervised learning, on the other hand, is to find patterns or structure in a set of unlabeled data, such as image compression. These types of learning can be defined more formally:
\begin{itemize}
\item \textbf{Supervised learning (classification, regression)}: \\
\indent Given $(x^1, y^1),...,(x^n, y^n)$ choose a function $f(x) = y$\newline
\indent $x_i$ = data point\newline
\indent $y_i$ = class/value/label

\item \textbf{Unsupervised learning (clustering, dimensionality reduction)}: \\
\indent Given $(x^1, x^2,...,x^n)$ find patterns in the data
\end{itemize}

\subsection{Types of Supervised Learning}
Here we focus our attention on supervised learning. Supervised learning can generally be split into two categories: classification and regression. In classification problems the goal is to output a discrete value, such as a 1 or 0 (whether or not the picture is a cat). Regression, on the other hand, attempts to learn a function that returns values in the continuous domain (such as house prices). 

In either type of problem, the goal is to learn the function $f(x)$ that best approximates the data. In order to evaluate how well a model approximates the data we define a loss (or cost) function that, when minimized, provides the optimal model parameters. There are many types of loss functions, but here we present a few of the most common for both regression and classification:

\begin{itemize}
\item \textbf{Regression (see Figure \ref{fig:regression_example})} \newline
\indent $\ell^2$ loss: $\sum_{i}|f(x^i) - y^i|^2$: Assigns a higher penalty to outliers \newline
\indent $\ell^1$ loss: $\sum_{i}|f(x^i) - y^i|$: Applies equal penalty to outliers 

\item \textbf{Classification (see Figure \ref{fig:classification_example})}\newline
\indent $0 - 1$ loss: $\sum_{i}\textbf{1}\{f(x^i)\neq y^i\}$: Non-differentiable \newline
\indent Cross entropy loss: -$\sum_{i}(y^i)^T$log$(f(x^i))$: Differentiable version of the $0 - 1$ loss
\end{itemize}

\begin{figure}[H]
	\centering
    \begin{subfigure}{0.45\linewidth}
    	\includegraphics[height=1.5in]{pics/lecture_8_regression.jpg}
        \caption{Regression}
        \label{fig:regression_example}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
    	\includegraphics[height=1.5in]{pics/lecture_8_classification.jpg}
        \caption{Classification}
        \label{fig:classification_example}
    \end{subfigure}
    \caption{Examples of regression and classification for supervised learning}
\end{figure}

\subsection{Parametric vs. Non-parametric Models}
Within machine learning models can also be defined as being either parametric or non-parametric models. Parametric models can entirely described by a set of learned parameters learned from training data. Once the parameters are learning (using gradient descent, for example), the model no longer needs access to the original training data. The expressivity of the model is therefore a direct result of the feature set and the model parameters. Non-parametric models, however, learn models that are dependent upon the original training data. K-nearest Neighbors, for example (an unsupervised learning algorithm) uses the actual data to categorize any additional data points, and is therefore non-parametric. Examples are shown in Figures \ref{fig:parametric_models} and \ref{fig:nonparametric_models} below. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.4\linewidth]{pics/lecture_8_linear_regression.jpg}
      \caption*{Linear regression}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.4\linewidth]{pics/lecture_8_linear_classifier.jpg}
      \caption*{Linear Classifier}
    \end{subfigure}
	\caption{Examples of parametric models}
    \label{fig:parametric_models}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.4\linewidth]{pics/lecture_8_spline_fitting.jpg}
      \caption*{Spline fitting}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.4\linewidth]{pics/lecture_8_k_nearest.jpg}
      \caption*{k-Nearest Neighbors}
    \end{subfigure}
    \caption{Examples of non-parametric models}
    \label{fig:nonparametric_models}
\end{figure}

\subsection{Finding the Optimal Model}
Once we define a model and a loss function, the final step is to find the best model parameters that minimize the loss. Sometimes an analytical solution can be found, such as with linear least squares:\newline
\[
\begin{bmatrix}
    y_1^1 & y_2^1 \\
    y_1^2 & y_2^2 \\
    \vdots \\
    y_1^n & y_2^n
\end{bmatrix}
= 
\begin{bmatrix}
    x_1^1 & x_2^1 & \dots & x_k^1\\
    x_1^2 & x_2^2 & \dots & x_k^2 \\
    \vdots & \vdots & \ddots & \vdots\\
    x_1^n & x_2^n & \dots & x_k^n     
\end{bmatrix} 
\begin{bmatrix}
    a_{11} & a_{12} \\
    a_{11} & a_{12}  \\
    \vdots & \vdots \\
    a_{k1} & a_{k2}      
\end{bmatrix}
\]
$$f_A(x) = xA, \ell^2 loss$$
$$\hat{A} = (X^TX)^{-1}X^TY$$ 

\noindent However, finding an analytical solution is often extremely difficult or computationally inefficient. In practice, machine learning relies on the relatively simple but powerful numerical optimization algorithm of gradient descent. Instead of updating parameters of the model by trial-and-error, the parameters are updated in the direction of greatest change (i.e., along the gradient).

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{pics/lecture_8_num_opt1.jpg}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{pics/lecture_8_num_opt2.jpg}
\end{subfigure}
\end{figure}

Typically, we then define a cost function $J$, which a combination of the loss function for each datapoint in the dataset:\newline

$$J = \frac{1}{n}\sum_{i=1}^n|f(x^i) - y^i|^2 = \frac{1}{n}\sum_{i=1}^nL_i$$

However, computing $\triangledown L$ could be very computationally intensive as it is not uncommon for training sets to have millions of samples. Alternatively, we can form an approximation using part, or a batch, of the training set. This technique is known as stochastic gradient descent and in practice works quite well.\newline

$$\triangledown L \approx \frac{1}{|S|}\sum_{i\in S \subset \{1,...,n\}} \triangledown L_i$$

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.5\linewidth]{pics/lecture_8_stochastic_grad_desc.jpg}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.5\linewidth]{pics/lecture_8_grad_desc_options.jpg}
    \end{subfigure}
    \caption{Examples of stochastic gradient descent. For ``batch'' gradient descent where the entire dataset is processed at the same time, the cost function $J$ should monotoncially decrease with each step. Stochastic gradient descent, however, will move in random directions but with an overall tendency towards the minimum. Stochastic gradient descent is not guaranteed to converge, but often does in practice. Often the use of ``mini-batches'' is used to balance the computational benefits of stochastic gradient descent and the stability of ``batch'' gradient descent, where small ``batches'' of the training set are processed at the same time (typically 32, 64, or 128). Even better performance can be achieved by using more advanced algorithms such as gradient descent with momentum, RMSprop, or the Adam Optimizer that combines these two. }
\end{figure}

\subsection{Regularization} \label{sec:regularization}
When the algorithm performs well on the training set but poorly in the real world (typically represented by a hold-out cross-validation set of unseen examples), we say the model is ``overfitting'' the training data. In other words, the model has learned the particular characteristics of the training dataset so well that it fits the noise as well as the general trend. To avoid overfitting the model to the training data (we want our model to generalize well to new samples), we may add additional terms to the loss function to penalize "model complexity". This is known as regularization.

\begin{itemize}
\item $\ell^2$ regularization: $||A||_2$ often corresponds to a Gaussian prior on parameters A. 
\item $\ell^1$ regularization: $||A||_1$ often encourages sparsity in A (easier to interpret/explain).
\end{itemize}

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth]{pics/lecture_8_knn_1.jpg}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth]{pics/lecture_8_knn_10.jpg}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth]{pics/lecture_8_knn_100.jpg}
  \end{minipage}
  \caption{Examples of regularization. When the regularization is too low or non-existent we see behavior shown in the figure on the left, where the model is fitting the ``noise'' of the data, and is clearly not generalizable (we call this high variance). When regularization is too high, as shown in the figure on the right, the model effectively reduces in complexity and approaches a simple linear classifier, and performs poorly on both the training and test datasets (we call this high bias). The figure in the middle shows a good balance between these extremes.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[height = 2in]{pics/lecture_8_regularization_loss.jpg}
\caption{Training and test set error. A typical plot for a model that over fits to the training set at the expense of generalization. As the model's performance on the training set increases, the performs on a new set of examples gets worse. Regularization helps overcome this behavior.}
\end{figure}

\subsection{Interpretation of Linear classifiers}
Linear models of the form $f(x_i,W,B) = Wx + b$ are commonly used in machine learning. Intuitively, we can think of this as assigning a weight to each feature (plus a bias term). For example, in predicting house prices we may believe that the number of rooms in a house is more important to the final selling price than the number of light switches, so we would assign more weight to the former feature. This same idea can be applied to each pixel value in an image to determine whether it is a cat.
\begin{figure}[H]
\centering
\includegraphics[height = 2in]{pics/lecture_8_parameters.jpg}
\caption{Using a linear classifier to classify an image. An image is typically a 3D matrix with height, width, and depth (having 3 colors--RGB). We can reshape a $h x w x d$ matrix to a vector of length $h*w*d$ and give the image class scores using a weighting matrix $W$ an bias vector $b$. The number of rows in $W$ and $b$ corresponds to the number of classes we are evaluating.}
\end{figure}

To learn the parameters of a (linear) model, large datasets are used. One example is the CIFAR-10 dataset which contains ten classes of images, each with 6000 examples. After a classifier is trained on such a dataset, each row of $W$ can be thought of as a "template" for nearest neighbor classification.
\begin{figure}[H]
\centering
\includegraphics[height = 2in]{pics/lecture_8_linear_classifier_interpretation.jpg}
\caption{CIFAR-10 dataset example}
\end{figure}

Continuing with the "template" idea, another way to think about linear classifiers is as dot products, which can be thought of as a measure of similarity. By padding the feature vector $x$ with a 1, we can include the bias term in our weights such that $f(x_i,W) = Wx$. The dot product of the class template with our image that produces the largest value should be the class of our image.

$$\omega^Tx = ||\omega||||x||cos(\theta) \rightarrow cos(\theta) = \frac{\omega^Tx}{||\omega||||x||}$$

We can also interpret the dot product as a projection: "how much" $\omega$ is in $x$ according to the component $x$ along $\omega$.
\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth]{pics/lecture_8_dot_prod_-1.jpg}
    \caption*{$cos(\theta) \approx -1$}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth]{pics/lecture_8_dot_prod_0.jpg}
    \caption*{$cos(\theta) \approx 1$}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth]{pics/lecture_8_dot_prod_1.jpg}
    \caption*{$cos(\theta) \approx 0$}
  \end{minipage}
  \caption{Cosine Similarity. The extrema are when two vectors are nearly opposite, very similar, or have limited project.  }
\end{figure}

\subsection{Softmax Regression}
Once we have found class scores it's often useful to normalize them so classes can be more easily compared. Outputs from the model can be turned into a probability vector over classes using the softmax function. The sum of all class scores will sum to 100\%.
\newline

\begin{align}
    p(y^i = j|x^i) = \frac{e^{x^iW_j + b_j}}{\sum_k e^{x^iW_k + b_k}},     			\sigma(z) &= \begin{bmatrix}
    \frac{e^{z_1}}{\sum_k e^{z_k}} \\
    \vdots \\
    \frac{e^{z_m}}{\sum_k e^{z_k}} \\
    \end{bmatrix}
\end{align}

Another common form for class scores comes from Cross-entropy loss computed against "one-hot" class vector $[0,...,1,...,0]$ where all but the correct class are assigned 0 and the correct class is assigned 1.\newline

\subsection{Generalizing linear models - Features}
Linear regression/classification can be very powerful when empowered by the right features. While the mechanics of the classifier remain linear, the features can be nonlinear via basis functions. For example, a nonlinear feature may be a modified original feature: $x^n$, $x_ix_j$, etc. For example, our original features may be voltage and current from a circuit, but the product of these is a new feature: power, that may be more informative to our model.\newline
\[
\begin{bmatrix}
    y_1 \\
    y_2 \\
    y_3 \\
    \vdots \\
    y_n
\end{bmatrix}
= 
\begin{bmatrix}
    1 & x_1 & x_1^2 & \dots & x_1^m\\
    1 & x_2 & x_2^2 & \dots & x_2^m\\
    1 & x_3 & x_3^2 & \dots & x_3^m\\
    \vdots & \vdots & \ddots & \vdots\\
    1 & x_n & x_n^2 & \dots & x_n^m\\     
\end{bmatrix} 
\begin{bmatrix}
    a_0 \\
    a_1  \\
    a_2 \\
    \vdots \\
    a_m      
\end{bmatrix}
\]
\begin{figure}[H]
\centering
\includegraphics[height = 2in]{pics/lecture_8_generalized_models.jpg}
\caption{Nonlinearity via basis functions. Higher order models can be fit using a linear model when nonlinear features are used.}
\end{figure}

One example of using the "right features" is demonstrated by eigenfaces, a means for facial recognition. Instead of classifying unprocessed images of faces, eigenvectors from a set of images can be used to reduce the dimensionality of the feature space of an image for classification.
\begin{figure}[H]
\centering
\includegraphics[height = 2in]{pics/lecture_8_eigenfaces.jpg}
\caption{Eigenfaces}
\end{figure}

Another very successful feature set for shape classification/regression is Histogram of Oriented Gradients (HOG). These human-designed features are quite successful at finding the outline of shapes by calculating the gradients of pixels.

\begin{figure}[H]
\centering
\includegraphics[height = 2in]{pics/lecture_8_HOG.jpg}
\caption{(Dalal and Triggs 2005). Left to right: test image, visualization of HOG descriptor, HOG descriptor scaled by positive weights, HOG descriptor scaled by negative weights.}
\end{figure}

While humans have been successful at feature extraction across many applications, it is possible to automatically extract features using neural networks that rely on gradient descent.
\begin{figure}[H]
\centering
\includegraphics[height = 2in]{pics/lecture_8_feature_extraction.jpg}
\caption{Comparison of classification using bespoke feature extraction vs end-end learning}
\end{figure}

%%%%%%%%%%%
% PART II %
%%%%%%%%%%%
\section{Neural Network Basics}
Work on Artificial Neural Networks, generally known as just 'Neural Networks', has been inspired by the fact that the human brain processes information in a completely different way than computers. Due to this, human brains are much more efficient at certain tasks, such as vision, language processing, etc. than computers. Artificial neural networks aim to mimic this biological model by employing a large number of simple interconnected processing units or 'neurons'.  We may thus offer the following definition of a neural network viewed as an adaptive machine\cite{haykin}:

A neural network is a massively parallel distributed processor made up of simple processing units that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects:\\
1. Knowledge is acquired by the network from its environment through a learning process.\\
2. Inter-neuron connection strengths, known as synaptic weights, are used to store the acquired knowledge.

\subsection{Perceptron - Analogy to a Biological Neuron}
The figure below shows a simplified diagram of a biological neuron:

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/neuron.png}
\caption{Simplified model of a biological neuron\cite{cs231n-website}}
\label{fig:bio-neuron}
\end{figure}

There are approximately 86 billion neurons in the human nervous system and they are connected with approximately $10^14$ - $10^15$ synapses\cite{cs231n-website}.

The figure below shows a commonly used mathematical model of a neuron:

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/neuron_model.jpeg}
\caption{Mathematical model of a neuron\cite{cs231n-website}}
\label{fig:neuron_model}
\end{figure}

Signals travel along the axons (e.g. x0) and interact multiplicatively (e.g. w0x0) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. w0). The idea is that the synaptic strengths (the weights w) are learnable and control the strength of influence (and its direction: excitatory (positive weight) or inhibitory (negative weight)) of one neuron on another. We can model the firing rate of the neuron with an activation function f, which represents the frequency of the spikes along the axon. Historically, a common choice of activation function was the sigmoid function, but the ReLU function is more commonly used nowadays. This is because the gradient of the sigmoid function becomes small as its value increases which reduces the training speed of the model. 


\subsection{Single Layer Network / Logistic Regression}
Inspired by this model of the neuron, a 1 layer neural net can be built which takes in binary inputs and provides binary output by taking a weighted sum and passing it through an activation function (as seen below). The theory is that if these weights are tuned perfectly, then it should be able to classify the inputs correctly. 


\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/Logistic_regression.png}
\caption{Single layer neural net\cite{Lecture slides}}
\label{fig:Single layer neural net}
\end{figure}


\subsection{Multi-layer Neural Network / Deep Learning}

We can get more resolution on our classifications by putting multiple of these layers together. In such regard, each layer can "learn" to classify a different part of the input. Take for an example the problem of classifying handwritten digits, in particular classifying the number 3. The first layer can be responsible for classifying a curved top, the second a curved bottom, etc. 

A sample is shown below (from : http://www.cs.cmu.edu/~aharley/vis/conv/flat.html)

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/deep_neural_net.png}
\caption{Deep Neural Net for Classifying Handwritten Numbers\cite{}}
\label{fig:Single layer neural net}
\end{figure}

Representing this is simply a bunch of single layer neural networks linked together where the output of 1 layer is the input of the second layer.

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/deep_learning.png}
\caption{Deep Learning\cite{}}
\label{fig:Deep Learning}
\end{figure}

\subsection{Activation Functions}

If we didn't use non-linear activation functions, the output of the neural network would just be a linear function of the input. Such a network is basically just a Linear regression Model and and no matter how many layer and nodes we add to such a network, it is equivalent to having a network with a single node.\\

Hence, we use Activation functions to make the network more powerful give it the ability to represent non-linear complex functional mappings between inputs and outputs.

Another important feature of an Activation function is that it should be differentiable. This is because we perform back-propagation to calculate the gradients of the Loss function with respect to the weights and then accordingly optimize weights using gradient descend or any other Optimization technique to decrease the Loss Function.

The following are the most commonly used activation functions:

\begin{enumerate}

\item \textbf{Sigmoid Function:}
The Sigmoid Function takes a real-valued number and maps it to a range between 0 and 1.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/sigmoid.jpeg}
\caption{The sigmoid function\cite{cs231n-website}}
\label{fig:sigmoid}
\end{figure}
\begin{equation*}
f(x) = \frac{1}{1+e^{-x}}
\end{equation*}
Sigmoid Activation functions have historically been used extensively but have now fallen out of favor due to the following drawbacks:
\begin{enumerate}
\item They are prone to saturate as the gradients are very small away from the center. When this occurs, almost no signal will flow through the neuron to its weights and recursively to its data.
\item They are not zero-centered.
\end{enumerate}

\item \textbf{Tanh function:}
The Tanh function looks quite similar to a sigmoid function. It maps a real-valued number to a range between -1 and 1.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/tanh.jpeg}
\caption{The tanh function\cite{cs231n-website}}
\label{fig:tanh}
\end{figure}
\begin{equation*}
f(x) = \tanh(x)
\end{equation*}
It saturates in the same way as a sigmoid does but it has the advantage of being zero-centered. Hence, in practice, it is always preferred to a sigmoid function.
 
\item \textbf{ReLU Function:}
The Rectified Linear Unit is simply a threshold at zero and has become very popular in the last few years.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/tanh.jpeg}
\caption{The ReLU function\cite{cs231n-website}}
\label{fig:ReLU}
\end{figure}
\begin{equation*}
f(x) = max( 0, x )
\end{equation*}
It has the following advantages:
\begin{enumerate}
\item It greatly accelerates the convergence of stochastic gradient descent compared to tanh/sigmoid.
\item It is computationally cheaper to implement than tanh/sigmoid as it can implemented by simply thresholding a matrix of activations at zero.
\end{enumerate}
The main disadvantage of the ReLU is that ReLU units can irreversibly die during training since they can get knocked off the data manifold.

\item \textbf{Leaky ReLU Function:}
The Leaky ReLU attempts to fix the problem of dying nuerons by providing a small negative slope in the $x<0$ region.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/Leaky.png}
\caption{The Leaky ReLU function\cite{cs231n-website}}
\label{fig:ReLU}
\end{figure}
\begin{equation*}
f(x) = max( 0.1 x, x )
\end{equation*}

\end{enumerate}
\subsection{Training Neural Networks}
We now need a way to teach these neural networks weights to make correct predictions. This is done by running the network on known data points and updating the weights of the neurons based on the correctness of the model in classifying the input. \\

Running this however on each input one at a time can take a long period of time for training. Instead these models are run on a sample batch of data. In particular the steps are the following.\\ 

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/training_net.png}
\caption{Training a neural net\cite{}}
\label{fig:Training a neural net}
\end{figure}

\textbf{1. Sample a batch of data.}
We can use a smaller batch of data at each iteration to train the model to decrease training time.

\textbf{2. Run the input it through the graph and compute the loss.} 
The loss function provides serves as a quality metric and provides us an idea on the error of our model. That is, how correct it is.

\textbf{3. Backpropagate} 
Activation functions are chosen to be differentiable to make this step easier. Backpropagation is done by taking the gradient with respect to the weights of each neuron in each layer. 

\textbf{4. Update these parameters using SGD (Stochastic Gradient Descent)}
Update each parameter using the gradient calculated in the previous step

\textbf{5. Repeat many times}

\subsection{Overfitting}
Creating complex models sometimes have the disadvantage of being extremely accurate only on the test data. That is, the model is able to classify its entire training data with high accuracy however is unable to generalize and fails with test / real data. 

On the other hand too simple models are unable to learn the more subtle trends in the data. 

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/overfitting.png}
\caption{Overfitting\cite{}}
\label{fig:Overfitting}
\end{figure}
http://mlwiki.org/index.php/Overfitting

In the example above the left model was too simple and was unable to classify some of the values correctly. On the other the right model despite perfectly classifying on this training dataset is unlikely to perform as well in real-world situations. The middle offers a good classification. 

To combat overfitting, regularizaiton is usually introduced in models. Regularization is a technique that penalizes complex models or prevents them from being made. Some of these techniques include

\textbf{Dropout}
Randomly select neurons that will not be activated at each pass during training. 

\textbf{L2 Regularization}
Penalize the square magnitude of all parameters which forces neuron size to be smaller. 

\textbf{Max Norm Constraints}
Limit the maximum weight of a neuron. 


%%%%%%%%%%%%
% PART III %
%%%%%%%%%%%%
\section{Convolutional Neural Networks}
A simple Convolutional Neural Networks is a sequence of layers, and every layer of a Convolutional Neural Networks transforms one volume of activations to another through a differentiable function. There are four main types of layers to build ConvNet architectures: \textbf{Convolutional Layer}, \textbf{Nonlinearity Layer}, \textbf{Pooling Layer}, and \textbf{Fully-Connected Layer}. These layers are stacked to form a full ConvNet architecture. In this way, Convolutional Neural Networks transform the original image layer by layer from the original pixel values to the final class scores. 

In this section, we mainly discuss the three types of layer in Convolutional Neural Networks, including Convolutional layer, Pooling layer and Fully-connected layer.

\subsection{Convolutional Layer}
\textbf{Parameter Sharing}
Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. It is hard to make perceptrons scalable when the input image size is large given the fact the number of parameters grow quickly with the input size. However, we can dramatically reduce the number of parameters by making one reasonable assumption. If we know the input is image data, we can assume some spatial symmetries. In other word, if one feature is useful to compute at some spatial position $(x,y)$, then it should also be useful to compute at a different position $(x_2,y_2)$, so same parameters could be used.

\textbf{Convolution Details}
The convolution operation essentially performs dot products between the filters and local regions of the input. In other words, each element is computed by element-wise multiplying the local regions of the input (blue part in Figure \ref{fig:conv_demo}) with the filter (red part in Figure \ref{fig:conv_demo}), summing it up, and then offsetting the result by the bias. The output is the green part in Figure \ref{fig:conv_demo}.

Noticing we give the demo in 2D format. In real cases, the input would be a 3D tensor while he depth of the output volume is a hyperparameter. It corresponds to the number of filters we would like to use, each learning to look for something different in the input. Take Figure \ref{fig:conv_demo2} as an example, if we have 6 $5\times5$ filters, we will get 6 separate activation maps as output.

\textbf{Spatial arrangement} Besides depth, there are two hyperparameters to control the size of the output volume: the stride and zero-padding
\begin{itemize}
\item \textbf{stride} Stride specifies how we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. 
\item \textbf{zero-padding} In some cases, the size of filters and stride don’t “fit” neatly and symmetrically across the input. For example, if the input size $H=W=10$ while the filter size is $F=3$ and it takes stride $S=3$, it would be impossible to use stride $S=2$, since 
\begin{equation}
\begin{aligned}
(H-F)/S+1&=(10-3)/3+1=3.33~\text{(Not an integer)}\\
(W-F)/S+1&=(10-3)/3+1=3.33~\text{(Not an integer)}\\
\end{aligned}
\end{equation}
Therefore, we need to use zero-padding. We add additional zero-paddings in the contour of the image. If we take zero-padding $P=1$, we would have 
\begin{equation}
\begin{aligned}
(H-F+2*P)/S+1&=(10-3+2*1)/3+1=4~\text{(An integer)}\\
(W-F+2*P)/S+1&=(10-3+2*1)/3+1=4~\text{(An integer)}\\
\end{aligned}
\end{equation}
\end{itemize} 

In general, setting zero padding to be $P=(F−1)/2$ when the stride is $S=1$ ensures that the input volume and output volume will have the same size spatially. It is very common to use zero-padding in this way.
 
\begin{figure}[!htb]
\centering
\includegraphics[height = 3in]{pics/cnn.png}
\caption{Convolution Demo\cite{cs231n-website}}
\label{fig:conv_demo}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/cnn2.png}
\caption{Number of filter and depth of output\cite{cs231n-website}}
\label{fig:conv_demo2}
\end{figure}
\subsection{Pooling Layer}
In practice, it is common to periodically insert a Pooling Layer in-between successive Conv layers in a ConvNet architecture. It is mainly used to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation load, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation if max pooling, AVG if average pooling, etc. The most common form is a pooling layer with filters of size $2\times2$ applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75\% of the activations. In this case, every pooling operation would be taking a max or average over 4 numbers (little $2\times2$ region in some depth slice). The depth dimension remains unchanged. In a nutshell, \textit{pooling layer \textbf{downsamples} the volume spatially, independently in \textbf{each} depth slice of the input volume.} 

\begin{figure}[!htb]
\begin{subfigure}[!htb]{0.45\linewidth}
\includegraphics[scale=0.33]{pics/pool}
\caption{general pooling}
\label{fig:genpool}
\end{subfigure}
\hfill
\begin{subfigure}[!htb]{0.54\linewidth}
\includegraphics[scale=0.33]{pics/maxpool}
\caption{max pooling}
\label{fig:maxpool}
\end{subfigure}
\caption{Pooling Layer\cite{cs231n-website}}
\end{figure}

The Fig.\ref{fig:genpool} shows how pooling works generally. A $2\times2$ pooling filter with stride 2 is applied on the $224\times224\times64$ input volume, yielding the $112\times112\times64$ output volume. Note that the width and height of the input volume shrinks to half while the depth is preserved. The most common downsampling operation is MAX. The Fig.\ref{fig:maxpool} here shows max pooling with a stride of 2. That is, each max is taken over 4 numbers in a $2\times2$ square. For instance, the top-left red square squashes to a single value, \texttt{\textbf{6}}, which is \texttt{\textbf{max\{1, 1, 5, 6\}}}.

\subsection{Fully-Connected Layer}
The last few layers of a ConvNet architecture are typically Fully-Connected (FC) Layers. As seen in regular neural networks, FC Layers serve as a linear classifier for classification or regression. Their activations can be computed with a matrix multiplication followed by a bias offset, \textit{i.e.} $f = Wx + b$. See the Neural Network Basics section of the note for more information.

\subsection{Mainstream Model Architectures}
Empowered with impressive ability of feature extraction and optimization, deep convolutional neural networks has become the cornerstone of data-driven visual recognition techniques nowadays. Since AlexNet's overwhelming success in ILSVRC 2012, a number of milestone model architectures have been proposed. The most common are:
\begin{itemize}
\item \textbf{AlexNet}\cite{alexnet}: Developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet significantly outperformed the second runner-up (top 5 error of 16\% compared to runner-up with 26\% error) ILSVRC challenge in 2012. It is the first work that popularized Convolutional Neural Networks in Computer Vision.
\item \textbf{VGGNet}\cite{vggnet}: Proposed by Karen Simonyan and Andrew Zisserman. The VGGNet was the runner-up in ILSVRC 2014. Its main contribution was in showing that the depth of the network is a critical component for good performance. Researchers and engineers tend to design deeper rather than shallower models ever since.
\item \textbf{GoogLeNet}\cite{googlenet}: As its name shows, it is authored by Szegedy \textit{et al.} from Google. GoogLeNet was the ILSVRC 2014 winner with main contribution in developing Inception Module that dramatically reduced the number of parameters in the network.
\item \textbf{ResNet}\cite{resnet}:  Developed by Kaiming He \textit{et al.} ResNet was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization, which is now known as Residual Module. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of January 2018).
\end{itemize}

There also exist more model architectures proposed for specific tasks, such as YOLO\cite{yolo} and Faster R-CNN\cite{faster_rcnn} in the field of object detection and localization.

% References
\bibliographystyle{unsrt}
\bibliography{references}
\end{document}