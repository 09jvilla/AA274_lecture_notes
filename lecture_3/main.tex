\documentclass[twoside]{article}

\usepackage[math]{kurier}
\usepackage[sc]{mathpazo}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\renewcommand{\sfdefault}{kurier}


\usepackage{graphics}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}


\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf \sffamily AA 274: Principles of Robotic Autonomy
                        \hfill Winter 2018} }
       \vspace{4mm}
       \hbox to 6.28in { {\sffamily{\Large \hfill Lecture #1: #2  \hfill}} }
       \vspace{2mm}
       % \hbox to 6.28in { {\it \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace*{4mm}
}

\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%
%document
\begin{document}
%modify this
\lecture{3}{Open-Loop and Closed-Loop Motion Control}{}
\section{Introduction}

This lecture discusses the main techniques in optimal control and trajectory optimization.

First, we will review constrained optimization by formulating the problem using the Lagrangian. Then we will explore two different methods in solving an optimal control problem: we will thoroughly explore some equations behind Indirect Methods, enabling its computation along with examples, and give an overview on another approach: Direct Methods.
Then we will explore differentially flat problems, and how these enable the use of algebraic equations in non-linear problem solving.
Next we will explore closed-loop control, and some implementations.
Finally, we will combine open-loop and closed-loop approaches by introducing trajectory tracking.

\section{Constrained Optimization}
Before presenting the optimal control problem it is first useful to briefly review a standard constrained optimization problem given by

\begin{equation} \label{const}
\begin{split}
\underset{u}{\text{min}} \:\: &f(x) \\
\text{subject to} \:\: &h_i(x) = 0, \quad i = 1,\dots,m \\
\end{split}
\end{equation}

Problem \ref{const} is solved by introducing the \textit{\textbf{Lagrangian}} and then generating the necessary conditions for optimality (NOC). The Lagrangian is given by

$$L(x,\lambda) = f(x) + \sum_{i=1}^{m}\lambda_i h_i(x)$$

and the necessary conditions for optimality are

\begin{equation} \label{noc}
\begin{split}
\nabla_x L(x^*,\lambda^*) &= 0 \\
\nabla_\lambda L(x^*,\lambda^*) &= 0 \\
\end{split}
\end{equation}

One way to think about the necessary conditions (Eq. \ref{noc}) for optimality is as a filter that takes in all points $x$ and outputs a subset of potential optimal points $x^*$. If several potential points exist that satisfy the NOC, these could be local optima.

\section{Optimal Control Problem}

\begin{equation} \label{optcont}
\begin{split}
\underset{u}{\text{min}} \:\: &h(x(t_f),t_f) + \int_{t_0}^{t_f} g(x(t),u(t),t) dt \\
\text{subject to} \:\: &\dot{x}(t) = a(x(t),u(t),t) \\
&x(t) \in \mathcal{X}, \quad u(t) \in \mathcal{U}
\end{split}
\end{equation}

In general, $x(t) \in \mathbb{â€¢}{R}^n$, $u(t) \in \mathbb{R}^m$, and the initial condition $x(0) = x_0$ is given. In the context of this course the constraint $x(t) \in \mathcal{X}$ is typically relaxed to just be $x(t) \in \mathbb{R}^n$. In other words there are no state constraints.

The problem given by Eq. \ref{optcont} contains many details. In the objective function, the term $h(x(t_f),t_f)$ represents a cost associated with the end condition. For example if you wanted to minimize time you could have $h = t_f$. Also in the objective function, the integral produces a path cost, or in other words produces penalties along the way from $t_0$ to $t_f$. This would be useful if you wanted to minimize fuel use: for example you could set $g = u^2$.

The first constraint in the problem defines the differential constraints that represent the system dynamics. These in general are nonlinear which is why they are represented as $a(x(t),u(t),t)$.

Problem \ref{optcont} is typically solved to yield an open-loop trajectory and control, which are denoted as $x^*(t)$ and $u^*(t)$. Note that these are entire trajectories through time, not just individual points as we saw in Eq. \ref{const}. Since the control is open loop we also typically write

$$u^*(t) = f(x(t_0),t)$$

to indicate that the control is a function of time only, and does not utilize any state feedback.

It is important to realize the difference between solving these optimal control problems and solving general optimization problems. As mentioned, in optimal control we are solving for \textit{trajectories}, and not just points. For any given set of boundary conditions there are an infinite number of possible trajectories, and with the dynamics constraints also involved this becomes a formidable task.

The tools used to solve this Problem \ref{optcont} are generally classified as \textit{\textbf{Indirect Methods}} and \textit{\textbf{Direct Methods}}.



\subsection{Indirect Methods}
%--------------------------Example of Indirect Methods--------------------------%

Given the control inputs and thus the control function (either an open loop or closed loop scenario) we look to optimize over, one can proceed with either a direct or indirect method for solving. A good thought process to follow for an indirect method is to \textit{\textbf{"first optimize, then discretize"}}. The indirect method works by imposing all NCOs, looking at the optimal functions of time we generated, and then selecting the one that is the best. Take the unconstrained finite dimensional case of a simple function f(x) that we look to optimize over $x \in \mathbb{R}^n$:

$$\underset{x}{\text{min}} \:\: \nabla_f(x) = 0 \\$$

Taking the gradient of my function and looking at those candidates that survive, one can determine which values would optimize their function (looking to minimize for our case). Our parallel infinite dimensional problem takes the solutions from NCOs as differential equations which should then be discretized to solve accordingly.

After a series of complicated derivations to establish an augmented cost function and referring to section \textbf{3.2} on how to generally handle constrained optimization, one can arrive at what is known as the Hamiltonian to solve our functions:

\begin{equation} \label{hamiltonian}
\begin{split}
H := g(x(t), u(t), t) + p^\textbf{T}(t)[a(x(t), u(t), t]
\end{split}
\end{equation}
where the first paranthetical term denotes the running cost, $p^\textbf{T}(t)$ denotes the vector of Lagrange Mutlipliers (one for each individual constraint),and the final term denoting the RHS of the differential equation.
From here we can declare the NCO's that our system requires for optimality:
\begin{equation} \label{hamiltonianNCOs}
\begin{split}
\dot{x}^*(t) &= \frac{\partial{H}}{\partial{p}} = (x^*(t), u^*(t), p^*(t), t)\\
\dot{p}^*(t) &= -\frac{\partial{H}}{\partial{x}} = (x^*(t), u^*(t), p^*(t), t)\\
0 &= \frac{\partial{H}}{\partial{u}} = (x^*(t), u^*(t), p^*(t), t)
\end{split}
\end{equation}

The first equation represents the derivative of the Hamiltonian with respect to your individual Lagrange Multipliers which interestingly enough gives back the kinematic state equations of the robot. The second equation is called the costate of your function - looking to optimize the function over continuous infinite time, the lagrange multipliers become functions of time. The final equation is the optimality condition which is simply the Hamiltonian differiniated with respect to the constraint producing an algebraic equation. This is \textbf{under the assumption that we have no state constraints} which is sufficient for our focus of the class. The tedious part of optimal control is now solving these equations: we have produced 2N optimization variables in $x^*(t)$ and $p^*(t)$ with one M varibale $u^*(t)$ totaling 2N + M variables alongside our 2N differinitable equations and M algebraic equation.

To solve we must impose boundary conditions on our function with respect to the robot's final position and time such that:

\begin{equation} \label{hamiltonianBCs}
\begin{split}
[\frac{\partial{h}}{\partial{x}}(x^*(t_f), t_f - p^*(t_f))]^T\delta{x_f}\ + [H(x^*(t_f), u^*(t_f), p^*(t_f), t_f + \frac{\partial{h}}{\partial{t}}(x^*(t_f), t_f)]\delta{x_f} = 0\\
\end{split}
\end{equation}

which produces four possible solutions to satisfy the equation:

$t_f$ = fixed and $x(t_f)$ = fixed \dots $\delta{t_f}$ = 0 and $\delta{x(t_f)}$ = 0

$$\textbf{BC}: x^*(t_0) = x_0$$
$$x^*(t_f) = x_f$$


$t_f$ = fixed and $x(t_f)$ = free \dots $\delta{t_f}$ = 0 and $\delta{x(t_f)}$ = arbitrary

$$\textbf{BC}: x^*(t_0) = x_0$$
$$[\frac{\partial{h}}{\partial{x}}(x^*(t_f), - p^*(t_f))] = 0$$


$t_f$ = free and $x(t_f)$ = fixed \dots $\delta{t_f}$ = arbitrary and $\delta{x(t_f)}$ = 0

$$\textbf{BC}: x^*(t_0) = x_0$$
$$x^*(t_f) = x_f$$
$$[H(x^*(t_f), u^*(t_f), p^*(t_f), t_f + \frac{\partial{h}}{\partial{t}}(x^*(t_f), t_f)]\delta{x_f} = 0$$



$t_f$ = free and $x(t_f)$ = free \dots $\delta{t_f}$ = arbitrary and $\delta{x(t_f)}$ = arbitrary

$$\textbf{BC}: x^*(t_0) = x_0$$
$$[\frac{\partial{h}}{\partial{x}}(x^*(t_f), - p^*(t_f))] = 0$$
$$[H(x^*(t_f), u^*(t_f), p^*(t_f), t_f + \frac{\partial{h}}{\partial{t}}(x^*(t_f), t_f)]\delta{x_f} = 0$$

The last scenario is what we call a free final time state and introduces an unkown variable to our system of equations making it a set of 2N + M + 1. Think of this as an optimal control problem where we look to minimize the cost of our function at any given final time. We look at a thorough example for clarification.

Let's consider an example of free final time. Consider a point that moves in one dimension. Control input of the system is acceleration of the point. Boundary conditions for positions and velocities at initial and final times are given, but the final time is not fixed.

\begin{equation}\label{ExampleSCAndBC}%State Constraints and Boundary Conditions
\ddot{x} = u, x(0) = 10, \dot{x} = 0, x(t_f) = 0, \dot{x}(t_f) = 0
\end{equation}
\begin{equation}\label{ExampleCostFunction}
J = \frac{1}{2}\alpha t_f^2 + \frac{1}{2} \int_{t_0}^{t_f} bu^2(t) \mathrm{d}t
\end{equation}

Cost function shown in \eqref{ExampleCostFunction} is typical in control problems. The final term in cost function tries to minimize total time to get back to origin, while the stagewise cost attempts to keep acceleration from being too high, which requires a lot of control effort. Analytical solution \eqref{ExampleAnalyticalSolution} of the final time is pretty reasonable. When $b$ increases, penalty on control effort will be more serious, which causes the acceleration to be relatively low and $t_f$ to be longer. When $\alpha$ increases, the final cost term will become dominant, so $t_f$ will become shorter.

\begin{equation}\label{ExampleAnalyticalSolution}
t_f = (1800b/\alpha)^{1/5}
\end{equation}

In order to solve the control problem, firstly we need to define state \textbf{z} and Hamiltonian. From the first equation of \eqref{ExampleSCAndBC} we have two state constraints (in which $x_1 = x$). Thus we have two Lagrange multipliers $p_1$ and $p_2$ correspondingly.

\begin{equation}\label{ExampleSC}%State Constraints
\begin{split}
\dot{x_1} &= x_2 \\
\dot{x_2} &= u
\end{split}
\end{equation}

According to the trick for free final time, we have a dummy state $r$ that is equal to $t_f$ with dynamics $\dot{r} = 0$. So state \textbf{z} has five elements.

\begin{equation}\label{ExampleState}
\textbf{z} = [z_1, z_2, z_3, z_4, z_5] ^T= [x_1^*, x_2^*, p_1^*, p_2^*, r^*]^T
\end{equation}

Then we form Hamiltonian $H$ based on \eqref{ExampleCostFunction} and \eqref{ExampleSC}.

\begin{equation}\label{ExampleH}
H = \frac{1}{2}bu^2 + p_1x_2 + p_2u
\end{equation}

We can get Hamiltonian equations from $H$.

\begin{equation}\label{ExampleHEqs}
\begin{split}
\dot{x}_1^* &= \frac{\partial{H}}{\partial{p_1}}(\textbf{x}^*(t),u^*(t),\textbf{p}^*(t),t)= x_2^* \\
\dot{x}_2^* &= \frac{\partial{H}}{\partial{p_2}}(\textbf{x}^*(t),u^*(t),\textbf{p}^*(t),t)= u^* \\
\dot{p}_1^* &= -\frac{\partial{H}}{\partial{x_1}}(\textbf{x}^*(t),u^*(t),\textbf{p}^*(t),t)= 0 \\
\dot{p}_2^* &= -\frac{\partial{H}}{\partial{x_2}}(\textbf{x}^*(t),u^*(t),\textbf{p}^*(t),t)=-p_1^* \\
0 &= \frac{\partial{H}}{\partial{u}}(\textbf{x}^*(t),u^*(t),\textbf{p}^*(t),t) = bu^* + p_2^*
\end{split}
\end{equation}

Boundary conditions include the original ones and substitution for free $t_f$ and fixed $\textbf{x}(t_f)$ problem.

\begin{equation}\label{ExampleBC1*}
x_1^*(0) = 10, x_2^*(0) = 0, x_1^*(t_f) = 0, x_2^*(t_f) = 0
\end{equation}
\begin{equation}\label{ExampleBC2*}
\begin{split}
H(\textbf{x}^*(t_f), u^*(t_f), \textbf{p}^*(t_f), t_f)+\frac{\partial{h}}{\partial{t}}(\textbf{x}^*(t_f),t_f) = 0 \\
\frac{1}{2}bu^*(t_f)^2 + p_1^*(t_f)x_2^*(t_f)+p_2^*(t_f)u^*(t_f) + \alpha t_f = 0
\end{split}
\end{equation}

When you put $u^* = -\frac{1}{b}p_2^*$ and $x_2^*(t_f) = 0$ into \eqref{ExampleBC2*}, you can get a more clear version of the last boundary equation.

\begin{equation}\label{ExampleBC2*Clear}
-\frac{1}{2b}p_2^*(t_f)^2 + \alpha t_f = 0
\end{equation}

After substitution of $u^* = -\frac{1}{b}p_2^*$ into other Hamiltonian equations in \eqref{ExampleHEqs}, we unify the notation of states to \textbf{z} and rescale time with $\tau = t/t_f$. BVP becomes
\begin{equation}\label{FinalBVP}
\frac{\mathrm{d}\textbf{z}}{\mathrm{d}\tau} = t_f\frac{\mathrm{d}\textbf{z}}{\mathrm{d}t}
= z_5\begin{bmatrix}
    0 & 1 & 0  & 0                & 0 \\
    0 & 0 & 0  & -\frac{1}{b} & 0 \\
    0 & 0 & 0  & 0                & 0 \\
    0 & 0 & -1 & 0                & 0 \\
    0 & 0 & 0  & 0                & 0 \\
\end{bmatrix} \textbf{z}
\end{equation}

After the same operation, BC become

\begin{equation}\label{FinalBC}
\begin{split}
z_1(0) = 10, z_2(0) = 0, z_1(1) &= 0, z_2(1) = 0, \\
-\frac{1}{2b}z_4(1)^2 + \alpha z_5(1) &= 0
\end{split}
\end{equation}

Then we can input these BVP and BC into Python to solve the problem.

\begin{thebibliography}{9}
\bibitem{lyupanov}
Aicardi, Casalino, Bicchi, Balestrino.
\textit{Closed loop steering of unicycle like vehicles via Lyapunov techniques}.
IEEE, Genoa, Italy, 1995.


\bibitem{lyupanovproof}
Wang, Zhanshan, Liu, Zhenwei, Zheng, Chengde
\textit{Qualitative Analysis and Control of Complex Neural Networks with Delays}.
Springer-Verlag Berlin Heidelberg,  2016.


\bibitem{dflatness}
Richard M Murray, \textit{Optimization-based control}. California Institute of Technology CA, 2009.

\bibitem{optcontrol}
D. K. Kirk. \textit{Optimal Control Theory: An introduction}. Dover Publications, 2004.

\bibitem{bvpstandardform}
Ascher, U., \& Russell, R. D. \textit{Reformulation of boundary value problems into ``standard'' form}. SIAM review, 23(2), 238-254, 1981.

\bibitem{ee363notes}
Notes from ee363 at Stanford University
\textit{https://stanford.edu/class/ee363/lectures/lyap.pdf}

\end{thebibliography}

\subsubsection*{Contributors}
Winter 2019: [Your Names Here]
\\
Winter 2018: Joseph Lorenzetti, Kenneth Hoffmann, Oriana Peltzer, Zhe Huang, William Mangram
\end{document}
